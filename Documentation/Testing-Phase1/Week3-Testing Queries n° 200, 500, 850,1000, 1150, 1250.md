# W3: Testing Queries 200, 500, 850,1000, 1150, 1250, 1350

Créé par: Nahrain Gtari
Heure de création: 1. Januar 2024 19:39

**Query 200:** Use RTDIP components to read from PythonDeltaSharingSource, transform using BaseRawToMDMTransformer, then write to PythonDeltaDestination 

```python
from rtdip_sdk.pipelines.sources.python.delta_sharing import PythonDeltaSharingSource
from rtdip_sdk.pipelines.transformers.spark.meters.base_raw_to_mdm import BaseRawToMDMTransformer
from rtdip_sdk.pipelines.destinations.python.delta import PythonDeltaDestination

source = PythonDeltaSharingSource("{/path/to/source/table}").read_batch()

transformer = BaseRawToMDMTransformer(source).transform_batch()

destination = PythonDeltaDestination(transformer, "{/path/to/destination/table}", mode="append").write_batch()
```

**Query200:** Use rtdip components to read from python delta sharing source, transform using base raw to mdm transformer, then write to python delta destination

```python
from rtdip.pipeline import Pipeline
from rtdip.components.sources.python.delta_sharing import PythonDeltaSharingSource
from rtdip.components.transformers.python.base_raw_to_mdm import BaseRawToMDMTransformer
from rtdip.components.destinations.python.delta import PythonDeltaDestination

pipeline = Pipeline()

# Step 1: Read from Python Delta Sharing source
source = PythonDeltaSharingSource(table_name="your_delta_sharing_table")
pipeline.add_component(source)

# Step 2: Transform using Base Raw to MDM transformer
transformer = BaseRawToMDMTransformer()
pipeline.add_component(transformer)

# Step 3: Write to Python Delta destination
destination = PythonDeltaDestination(table_name="your_delta_table")
pipeline.add_component(destination)

# Connect the components
pipeline.connect(source, transformer)
pipeline.connect(transformer, destination)

# Run the pipeline
pipeline.run()
```

**Query 500:** how to take RTDIP components for reading from DataBricksAutoLoaderSource, transform for example using SEMJsonToPCDMTransformer, then write to SparkEventhubDestination

```python
from rtdip_sdk.pipelines.sources.spark.databricks_autoloader import DataBricksAutoLoaderSource
from rtdip_sdk.pipelines.transformers.spark.sem_json_to_pcdm import SEMJsonToPCDMTransformer
from rtdip_sdk.pipelines.destinations.spark.eventhub import SparkEventhubDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility
import json

def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    autoloader_source_configuration = {
        "databricks.autoloader.path": "{path/to/data}",
        "databricks.autoloader.fileFormat": "{file_format}",
        "databricks.autoloader.schema": "{schema}",
    }

    source_df = DataBricksAutoLoaderSource(spark, autoloader_source_configuration).read_batch()
    transformed_df = SEMJsonToPCDMTransformer(source_df).transform()

    eventhub_destination_configuration = {
        "eventhubs.connectionString": "{EventhubConnectionString}",
    }

    SparkEventhubDestination(
        spark, data=transformed_df, options=eventhub_destination_configuration
    ).write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 850:** how to RTDIP components to read from SparkDeltaSharingSource, take OPCPublisherOPCUAJsonToPCDMTransformer as a transform, then put in SparkDeltaDestination

```python
from rtdip.sdk.pipeline import Pipeline
from rtdip.sdk.components.spark.sources import SparkDeltaSharingSource
from rtdip.sdk.components.spark.transformers import OPCPublisherOPCUAJsonToPCDMTransformer
from rtdip.sdk.components.spark.destinations import SparkDeltaDestination

# Create a pipeline
pipeline = Pipeline()

# Add a SparkDeltaSharingSource to read data from a Delta Table with Delta Sharing enabled
source = SparkDeltaSharingSource("delta-sharing-table-name")
pipeline.add_component(source)

# Add a OPCPublisherOPCUAJsonToPCDMTransformer to transform the data
transformer = OPCPublisherOPCUAJsonToPCDMTransformer()
pipeline.add_component(transformer)

# Add a SparkDeltaDestination to write the transformed data to a Delta table
destination = SparkDeltaDestination("delta-table-name")
pipeline.add_component(destination)

# Set the connections between the components
pipeline.set_connections(source, transformer)
pipeline.set_connections(transformer, destination)

# Run the pipeline
pipeline.run()
```

**Query 1000:** I want to use RTDIP components to read from SparkEventhubSource, transform using BaseRawToMDMTransformer, then write to SparkPCDMToDeltaDestination

```python
from rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource
from rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm import BaseRawToMDMTransformer
from rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta import SparkPCDMToDeltaDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility

def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    # Read from SparkEventhubSource
    eventhub_source_configuration = {
        "eventhubs.connectionString": "{EventhubConnectionString}",
        "eventhubs.consumerGroup": "{EventhubConsumerGroup}",
        "eventhubs.startingPosition": '{"offset": "-1", "seqNo": -1, "enqueuedTime": None, "isInclusive": True}'
    }
    raw_data = SparkEventhubSource(spark, eventhub_source_configuration).read_batch()

    # Transform using BaseRawToMDMTransformer
    transformed_data = BaseRawToMDMTransformer(raw_data).transform()

    # Write to SparkPCDMToDeltaDestination
    destination_configuration = {
        "path": "{DeltaTablePath}",
        "mode": "overwrite",
        "merge": False
    }
    SparkPCDMToDeltaDestination(spark, transformed_data, destination_configuration).write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 1000:** I want to use rtdip components to read from spark eventhub source, transform using base raw to mdm transformer, then write to spark pcdm to delta destination

```python
from rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource
from rtdip_sdk.pipelines.transformers.spark.base_raw_to_mdm import BaseRawToMDMTransformer
from rtdip_sdk.pipelines.destinations.spark.pcdm_to_delta import SparkPCDMToDeltaDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility

def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    # Read from Spark Eventhub Source
    eventhub_source_configuration = {
        "eventhubs.connectionString": "{EventhubConnectionString}",
        "eventhubs.consumerGroup": "{EventhubConsumerGroup}",
        "eventhubs.startingPosition": json.dumps(
            {"offset": "0", "seqNo": -1, "enqueuedTime": None, "isInclusive": True}
        ),
    }
    source_df = SparkEventhubSource(spark, eventhub_source_configuration).read_batch()

    # Transform using Base Raw to MDM Transformer
    transformed_df = BaseRawToMDMTransformer(source_df).transform()

    # Write to Spark PCDM to Delta Destination
    destination_options = {}
    destination_path = "{path/to/delta/table}"
    SparkPCDMToDeltaDestination(spark, transformed_df, destination_options, destination_path).write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 1150:** I would like to use RTDIP components to read from SparkEventhubSource, transform using ECMWFExtractPointToWeatherDataModel, then write to SparkKafkaDestination 

```python
from rtdip_sdk.pipelines.sources.spark.eventhub import SparkEventhubSource
from rtdip_sdk.pipelines.transformers.spark.binary_to_string import BinaryToStringTransformer
from rtdip_sdk.pipelines.transformers.spark.ecmwf.nc_extractpoint_to_weather_data_model import ECMWFExtractPointToWeatherDataModel
from rtdip_sdk.pipelines.destinations.spark.kafka import SparkKafkaDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility
def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    eventhub_source_configuration = {
        "eventhubs.connectionString": "{EventhubConnectionString}",
        "eventhubs.consumerGroup": "{EventhubConsumerGroup}",
        "eventhubs.startingPosition": json.dumps(
            {"offset": "0", "seqNo": -1, "enqueuedTime": None, "isInclusive": True}
        ),
    }

    # Read from SparkEventhubSource
    source_df = SparkEventhubSource(spark, eventhub_source_configuration).read_batch()

    # Transform Binary to String
    string_df = BinaryToStringTransformer(source_df, "body", "body").transform()

    # Perform ECMWF extraction to weather data model
    extract = ECMWFExtractPointToWeatherDataModel(
        lat={latitude}, lon={longitude}, date_start=date_start, date_end=date_end
    )
    weather_df = extract.transform(string_df)

    # Write to SparkKafkaDestination
    kafka_destination_configuration = {
        "kafka.bootstrap.servers": "{KafkaBootstrapServers}",
        "topic": "{KafkaTopic}",
    }
    SparkKafkaDestination(
        spark, data=weather_df, options=kafka_destination_configuration
    ).write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 1250:**  use RTDIP components to read from SparkIoThubSource, transform using OPCPublisherOPCUAJsonToPCDMTransformer, then write to SparkKafkaEventhubDestination  ( form: Hi, can you ….? -> does not work)

```python

from rtdip_sdk.pipelines.sources.spark.iothub import SparkIoTHubSource
from rtdip_sdk.pipelines.transformers.spark.opc_publisher_opcua_json_to_pcdm import OPCPublisherOPCUAJsonToPCDMTransformer
from rtdip_sdk.pipelines.destinations.spark.kafka_eventhub import SparkKafkaEventhubDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility

def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    iothub_source_configuration = {
        "iothub.connectionString": "{IoTHubConnectionString}",
        "iothub.consumerGroup": "{IoTHubConsumerGroup}",
        "iothub.startingPosition": "{StartingPosition}",
    }

    source_df = SparkIoTHubSource(spark, iothub_source_configuration).read_batch()
    pcdm_df = OPCPublisherOPCUAJsonToPCDMTransformer(source_df, "body").transform()

    kafka_eventhub_destination_configuration = {
        "kafka.bootstrap.servers": "{KafkaBootstrapServers}",
        "eventhubs.connectionString": "{EventhubConnectionString}",
        "eventhubs.partitionKeyColumn": "{PartitionKeyColumn}",
    }

    SparkKafkaEventhubDestination(
        spark, data=pcdm_df, options=kafka_eventhub_destination_configuration
    ).write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 1350:**  use RTDIP components to read from SparkIoThubSource, transform using ECMWFExtractPointToWeatherDataModel, then write to SparkKinesisDestination

```python
from rtdip_sdk.pipelines.sources.spark.iothub import SparkIoTHubSource
from rtdip_sdk.pipelines.transformers.spark.ecmwf.extract_point_to_weather_data_model import ECMWFExtractPointToWeatherDataModel
from rtdip_sdk.pipelines.destinations.spark.kinesis import SparkKinesisDestination
from rtdip_sdk.pipelines.utilities import SparkSessionUtility

def pipeline():
    spark = SparkSessionUtility(config={}).execute()

    iothub_source = SparkIoTHubSource(spark=spark, eventHubConnectionString="{EventHubConnectionString}")
    df = iothub_source.read_batch()

    extract_transformer = ECMWFExtractPointToWeatherDataModel(
        lat_col="{latitudeColumn}",
        lon_col="{longitudeColumn}",
        ec_vars=["{variable1}", "{variable2}", "{variable3}"],
        date_col="{dateColumn}",
        time_col="{timeColumn}",
    )
    transformed_df = extract_transformer.transform(df)

    kinesis_destination = SparkKinesisDestination(
        data=transformed_df,
        options={
            "streamName": "{streamName}",
            "region": "{region}",
        },
    )
    kinesis_destination.write_batch()

if __name__ == "__main__":
    pipeline()
```

**Query 1350:**  I want RTDIP components to read from SparkIoThubSource, transform using ECMWFExtractPointToWeatherDataModel, then write to SparkKinesisDestination

```python
from rtdip_sdk.pipelines.sources.spark.iothub import SparkIoTHubSource
from rtdip_sdk.pipelines.transformers.spark.ecmwf.extract_point_to_weather_data_model import ECMWFExtractPointToWeatherDataModel
from rtdip_sdk.pipelines.destinations.spark.kinesis import SparkKinesisDestination
iothub_source_configuration = {
    "iothub.connectionString": "{IoTHubConnectionString}",
    "iothub.consumerGroup": "{IoTHubConsumerGroup}",
    # Add any other required parameters
}
iothub_source = SparkIoTHubSource(spark, iothub_source_configuration)
source_data = iothub_source.read_batch()
latitude = 40.7128
longitude = -74.0060
# Add any other required parameters
extract_model = ECMWFExtractPointToWeatherDataModel(
    latitude=latitude,
    longitude=longitude,
    # Add any other required parameters
)
transformed_data = extract_model.transform(source_data)
kinesis_destination_configuration = {
    "kinesis.streamName": "{KinesisStreamName}",
    # Add any other required parameters
}
kinesis_destination = SparkKinesisDestination(
    data=transformed_data,
    options=kinesis_destination_configuration,
)
kinesis_destination.write_batch()
```